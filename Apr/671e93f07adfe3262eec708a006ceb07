Apple is releasing a child safety feature that will detect nudity in messages using artificial intelligence (AI). The feature was launched in the US last year and is now expanding to the Message apps on iOS, iPadOS, and macOS in the UK, Canada, New Zealand, and Australia. The new ‘Communication Safety’ feature will allow parents to turn on warnings on their children’s iPhones so that all photos sent or received by the child on Apple Message will be scanned for nudity. Once enabled, if nudity is found in photos received by a child, the photo will be blurred, and the child will be warned that it may contain sensitive content and nudged towards resources from child safety groups. ‘You’re not alone, and can always get help from someone you trust or with trained professionals,’ says the the pop-up message. ‘You can also block this person.’ A similar response is triggered If nudity is found in photos sent by a child, nudging them to not to send the images, and an option to ‘Message a Grown-Up’. ‘Messages uses on-device machine learning to analyze image attachments and determine if a photo appears to contain nudity,’ said Apple. This means that feature Apple doesn’t get access to the photos as the scanning is done on the device itself. ‘The feature is designed so that no indication of the detection of nudity ever leaves the device. Apple does not get access to the messages, and no notifications are sent to the parent or anyone else,’ said Apple in a statement. Apple initially had plans of automatically alerting parents if children under 13, sent or received images containing nudity but seems to have scrapped them in the final release. Instructions on how to enable the feature is integrated with Apple’s existing Family Sharing system. Parents can turn on communication safety at in the Screen Time settings for their child’s account. In addition to this feature, Apple is also expanding the rollout of a new feature for Spotlight, Siri, and Safari searches that will point users toward safety resources if they search for topics relating to child sexual abuse. Last August, Apple announced a third initiative that involved scanning photos for child sexual abuse material (CSAM) before they’re uploaded to a user’s iCloud account which has been delayed due to privacy concerns. MORE : Scam apps on Apple’s macOS App Store won’t let you quit until you pay a subscription MORE : Facebook may be underreporting images of child abuse, says report